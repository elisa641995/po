{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisa641995/po/blob/main/skin_care.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1Qv8e5h2wSy"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "import torch\n",
        "from typing import Dict, List, Optional, Union, Tuple, Callable\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timeit\n",
        "import string\n",
        "import random\n",
        "import zipfile\n",
        "import datetime\n",
        "import textwrap\n",
        "#import torchtext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import scipy.stats as st\n",
        "import missingno as msno\n",
        "from tqdm.auto import tqdm\n",
        "from rich.text import Text\n",
        "import plotly.express as px\n",
        "from tensorflow import keras\n",
        "from bs4 import BeautifulSoup\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from rich.console import Console\n",
        "from sklearn.utils import shuffle\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "import matplotlib.ticker as ticker\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.models import Sequential\n",
        "#from torchtext import data, datasets\n",
        "from keras.layers import Dense, Dropout\n",
        "from IPython.display import display, HTML\n",
        "#from data_profiling import ProfileReport\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from matplotlib.ticker import NullFormatter\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from IPython.display import HTML, display, Markdown\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense, Dropout\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertTokenizer, BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.metrics import f1_score\n",
        "nltk.download('stopwords')\n",
        "sns.set(color_codes = True)\n",
        "sns.set_palette(palette = 'RdGy', n_colors = 8)\n",
        "colors = [\"#C20000\", \"#D65F5F\", \"#EBA3A3\", \"#F3C1C1\", \"#E1E1E1\", \"#B0B0B0\", \"#7F7F7F\", \"#4C4C4C\"]\n",
        "sns.palplot(sns.color_palette(colors))\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "UM43dgBV4hm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9mDzfV2U7Aw"
      },
      "source": [
        "##Exploratory data analysis of the product data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8EcNZ2TGoXF"
      },
      "outputs": [],
      "source": [
        "data_product.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGe-GLP8G_4K"
      },
      "outputs": [],
      "source": [
        "#Precentage of null values in every column\n",
        "data_product.isnull().sum()*100/data_product.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhxwTc0fN3oP"
      },
      "source": [
        "I recommend dropping the following columns from the dataset, as they contain a significant amount of missing values: `child_max_price`, `child_min_price`, `value_price_usd`, `sale_price_usd`, and `variation_desc`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31ZNz-Xi5Ja-"
      },
      "outputs": [],
      "source": [
        "data_product.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I will focus specifically on skincare products from Sephora. To ensure accurate analysis and insights, I will separate the skincare-related data from other categories, such as makeup. This approach will allow for a more precise evaluation of trends and performance within the skincare segment."
      ],
      "metadata": {
        "id": "NUL5RV947P7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssm8LLHsHqw4"
      },
      "outputs": [],
      "source": [
        "string_to_check =['Remover' , 'remover' , 'Cleanser' , 'cleanser' , 'Make up cleanser' , 'bb cream' , 'bbcream' , 'BB' , 'Mask' , 'Masks' , 'mask' , 'Lip balm' , 'lip balm' , 'balm' , 'Gel' , 'gel' , 'make up remover' , 'Make up remover' ,'Hairdresser', 'Curl','Repair','Thickening',\"Hairdresser\",'Heat' ,'shampoo','Shampoo','hair','haircare','Haircare','Style','Styler','style','scalp','Scalp','Conditioner','conditioner','Frizz']\n",
        "data_product = data_product[data_product['product_name'].str.contains('|'.join(string_to_check))]\n",
        "data_product.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCiJ3jbbKXPD"
      },
      "outputs": [],
      "source": [
        "data_product.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for duplicates in the data\n",
        "data_product.duplicated().sum()"
      ],
      "metadata": {
        "id": "DckzfvPM7nM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data type of the columns\n",
        "print(data_product.dtypes)"
      ],
      "metadata": {
        "id": "8PLTXd2u7yXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let’s begin uncovering the story the data has to share with us."
      ],
      "metadata": {
        "id": "xRqPvbJB9YSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [['John', 50, 'Male', 'Austin', 70],\n",
        "        ['Cataline', 45 ,'Female', 'San Francisco', 80],\n",
        "        ['Matt', 30 ,'Male','Boston', 95]]\n",
        "\n",
        "# Column labels of the DataFrame\n",
        "columns = ['Name','Age','Sex', 'City', 'Marks']\n",
        "\n",
        "# Create a DataFrame df\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "df['Sex'].value_counts()"
      ],
      "metadata": {
        "id": "hZMA1gQ6ed4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PiExJHhMr3b"
      },
      "outputs": [],
      "source": [
        "columns_to_plot = [ 'rating', 'price_usd', 'child_max_price', 'child_min_price']\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, column in enumerate(columns_to_plot, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    plt.hist(data_product[column].dropna(), bins=30, color='blue', alpha=0.7)\n",
        "    plt.title(f'Histogram of {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVXDFRWcPSsK"
      },
      "source": [
        "As we can see, the majority of the products have a rating above 3.5, indicating that customers are generally satisfied with their purchases. Additionally, the prices for skincare products typically fall within a range of up to $80, with only a few products exceeding that price point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bFvL66xvAL-"
      },
      "outputs": [],
      "source": [
        "#checkin if there is multucoluniarity in the data using VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "columns_to_multi = [ 'rating', 'price_usd', 'sale_price_usd', 'child_max_price', 'child_min_price','loves_count','price_usd','sale_price_usd','child_max_price','child_min_price']\n",
        "X = data_product[columns_to_multi].dropna()\n",
        "\n",
        "# Calculate VIF for each column\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining the correlations between the columns"
      ],
      "metadata": {
        "id": "7aMP3fevDxvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ypr15_k2UWx"
      },
      "outputs": [],
      "source": [
        "columns_to_multi = [ 'rating', 'price_usd', 'sale_price_usd', 'child_max_price', 'child_min_price','loves_count','price_usd','sale_price_usd','child_max_price','child_min_price']\n",
        "df_selected = data_product[columns_to_multi].dropna()\n",
        "corr_matrix = df_selected.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1VuAzJF3PKr"
      },
      "source": [
        "It is evident that there are significant correlations between various variables related to product pricing in the data. For instance, there's a strong positive correlation between `min_child_price` and `sale_price_usd`. This suggests that as the original product's price increases, the minimum price of the child product also tends to rise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGUHoxsyuUN6"
      },
      "source": [
        "Investigating Product Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvgWd9UIuTDj"
      },
      "outputs": [],
      "source": [
        "data_product['primary_category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1lgo0z323KA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.kdeplot(data=data_product, x='rating', hue='primary_category', common_norm=False, fill=True, alpha=0.5)\n",
        "plt.title('Distribution of Ratings by Primary Category (KDE)')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution varies by category, but overall, the majority of ratings seem to cluster around 4 and 5, indicating that products in these categories generally receive high ratings.Categoris Fragrance, Hair & Men This category have a relatively narrow distribution, peaking around 4.5-5. Its density is higher, indicating that most of the ratings are clustered around the high end (4.5 to 5), with few ratings below 4."
      ],
      "metadata": {
        "id": "9LNTrDCcqdfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.kdeplot(data=data_product, x='price_usd', hue='primary_category', common_norm=False, fill=True, alpha=0.5, bw_adjust=0.5)\n",
        "plt.xlim(0, 350)\n",
        "plt.title('Distribution of price $ by Primary Category (KDE)')\n",
        "plt.xlabel('Price (USD)')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "spXXkzsVry1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fragrance and skincare products exhibit a broader price distribution compared to most other categories. While both peak below 50, fragrance products show a long tail that extends beyond 250, reflecting a wide price range, including many high-end items. Hair, makeup, and men’s products, on the other hand, have sharper peaks around 50 and 30, respectively, but their distributions are much narrower, rapidly tapering off after $50. This suggests that the majority of products in these categories are concentrated in the lower price range, typically under 50 .\n",
        "\n",
        "Mini Size products stand out with the sharpest peak, around 10, and the narrowest distribution. This indicates that these items are tightly priced, mostly under $20, with very few products priced higher."
      ],
      "metadata": {
        "id": "Xl_5cHLgaQ1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruVCKZ-sUADe"
      },
      "outputs": [],
      "source": [
        "columns_to_plot=['child_min_price','sale_price_usd','child_max_price']\n",
        "fig = plt.figure(figsize=(25, 20))\n",
        "for i,col in enumerate(columns_to_plot):\n",
        "  ax = fig.add_subplot(5, 3, i + 1)\n",
        "  sns.kdeplot(data=data_product, x=col, hue='primary_category', common_norm=False, fill=True, alpha=0.5)\n",
        "  ax.set_title(f'Distribution of {col}  by Primary Category (KDE)')\n",
        "  ax.set_xlabel(f'{col}')\n",
        "  ax.set_ylabel('Density')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As noted earlier, the fragrance category shows a broader price distribution for its subcategories, covering a wide range of minimum and maximum prices. In contrast, skincare products exhibit a narrower distribution among their subcategories, while hair products display a wider spread. Mini size, makeup, and bath products all have a more concentrated price range, with peaks around 20-30 dollars, indicating that most of the subcategory products fall within this price bracket."
      ],
      "metadata": {
        "id": "1uEzOKpVi7-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKo2yY86bk8P"
      },
      "outputs": [],
      "source": [
        "#measure skewnes of dara\n",
        "data_product[columns_to_plot].skew(axis = 0, skipna = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zZ8v4yERKqW"
      },
      "source": [
        "Looking for some outlires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj2e8g5cgDmn"
      },
      "outputs": [],
      "source": [
        "columns_to_plot_box=['rating','price_usd','sale_price_usd','child_min_price','child_max_price']\n",
        "for i,col in enumerate(columns_to_plot_box):\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    sns.boxplot(x=data_product[col])\n",
        "    quantiles = np.quantile(data_product[col].dropna(), [0.25, 0.5, 0.75])\n",
        "    mean = data_product[col].dropna().mean()\n",
        "    title = f\"Distribution of {col}\\n\"\n",
        "    title += f\"Q1: {quantiles[0]:.2f}, Median: {quantiles[1]:.2f}, Q3: {quantiles[2]:.2f}, Mean: {mean:.2f}\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(f'{col}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwh2Szk_T95H"
      },
      "source": [
        "When examining the box plot, we notice several data points that might initially appear as outliers. However, considering the context—consumer reviews and sale prices—these points may not truly be outliers. Consumer opinions can vary widely, as each individual has unique preferences, meaning their reviews shouldn’t be dismissed as anomalies. Similarly, high sale prices for particularly successful or premium products, like those in the fragrance category, shouldn’t be seen as outliers either. As we’ve observed, fragrance products often have a broader price range, which can lead to the \"outliers\" displayed on the box plot. One exception is a product priced above 1,750, which belongs to the skincare brand Shani Darden. After a quick check on their website, their products typically range up to $200, suggesting this is likely a typo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEm4N6LFY2CO"
      },
      "outputs": [],
      "source": [
        "num_limited_products_primery_category = data_product.groupby('primary_category')['limited_edition'].sum()\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=num_limited_products_primery_category.index, y=num_limited_products_primery_category.values)\n",
        "plt.title('Number of Limited Edition Products by Primary Category')\n",
        "plt.xlabel('Primary Category')\n",
        "plt.ylabel('Number of Limited Edition Products')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcSZiquHZykq"
      },
      "source": [
        "We can observe that limited edition products are more commonly found in the primary categories of hair, makeup, and skincare. Now, let's explore whether there is a relationship between a product being labeled as a limited edition and its price."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "x=data_product['limited_edition']\n",
        "y=data_product['price_usd']\n",
        "stats.pointbiserialr(x, y)\n",
        "\n"
      ],
      "metadata": {
        "id": "BqabM56c81go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there is no significant correlation between a product being categorized as a limited edition and its price."
      ],
      "metadata": {
        "id": "k4GoUMQw_a0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjk699LJfkGr"
      },
      "outputs": [],
      "source": [
        "#Let's take a closer look at the top-rated and most expensive brands in the skincare.\n",
        "skin_brands = data_product[(data_product['primary_category'] == 'Skincare')&(data_product['brand_id'] !=6314)]\n",
        "expensive_best_rating_brands = skin_brands.groupby('brand_name').agg({'price_usd': 'median', 'rating': 'median'})\n",
        "top_brands = expensive_best_rating_brands.sort_values(by=['price_usd','rating'], ascending=False).head(10)\n",
        "top_brands.plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Top ten brands by median price and rating')\n",
        "plt.xlabel('brands')\n",
        "plt.ylabel('Median Value')\n",
        "plt.xticks(rotation=45,fontsize=8)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5E9VC9y_24F"
      },
      "source": [
        "##Reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_eUH1c3_4ou"
      },
      "outputs": [],
      "source": [
        "df_1=pd.read_csv('/content/reviews_0-250.csv')\n",
        "df_2=pd.read_csv('/content/reviews_1250-end.csv')\n",
        "df_3=pd.read_csv('/content/reviews_250-500.csv')\n",
        "df_4=pd.read_csv('/content/reviews_500-750.csv')\n",
        "df_5=pd.read_csv('/content/reviews_750-1250.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cROBKUwhAyq2"
      },
      "outputs": [],
      "source": [
        "df_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB_Q5o7dCIoX"
      },
      "outputs": [],
      "source": [
        "#lets look at the shapes of all the reviews data frames\n",
        "list_of_dfs=[df_1,df_2,df_3,df_4,df_5]\n",
        "for i in list_of_dfs:\n",
        "  print(i.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg4NjraxDdjK"
      },
      "outputs": [],
      "source": [
        "#combine the all the data frames one under the other\n",
        "all_reviews=pd.concat(list_of_dfs,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wozEUPcyDr6x"
      },
      "outputs": [],
      "source": [
        "#checking the concatination was done corectly\n",
        "num_rows=0\n",
        "for i in list_of_dfs:\n",
        "  num_rows+=i.shape[0]\n",
        "print(num_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q458vpG0DoZw"
      },
      "outputs": [],
      "source": [
        "print(all_reviews.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro3GiNe8D6dm"
      },
      "outputs": [],
      "source": [
        "#Let's examine the percentage of NaN values in each column of the all_reviews dataset.\n",
        "all_reviews.isnull().sum() * 100 / len(all_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U703XRPqREqy"
      },
      "outputs": [],
      "source": [
        "all_reviews['rating'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtixmYXwu9WX"
      },
      "outputs": [],
      "source": [
        "#quick checking some reviews of ratign 3 like 20 examples  , kust the review text make google colab present the whole sentence\n",
        "\n",
        "# Adjust display settings to show the full text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Now display the reviews\n",
        "all_reviews[all_reviews['rating'] == 3][['review_text','rating']].sample(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auT9ipj4xwQJ"
      },
      "source": [
        "When performing sentiment analysis to classify feedback into two categories (good/bad), or even three, I found that comments with a 3-star rating often tell an interesting story. After reviewing these reviews, I, as a potential buyer, would likely avoid purchasing these products. Many of the comments suggest that the products don't necessarily solve the issue, or that similar quality can be found at a lower price. While these products aren't considered bad, the level of satisfaction expressed isn't strong enough to convince me to buy them. When using sentiment analysis to predict whether customers will repurchase a product or if it's worth continuing production, items with a 3-star rating wouldn't fall into the \"I'm buying it\" category.There for I decided to have just 2 categories - good/bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByHcxDPFoJgu"
      },
      "outputs": [],
      "source": [
        "#sentiment column\n",
        "all_reviews['sentiment'] = all_reviews['rating'].apply(lambda x: 0 if x <= 3 else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqKt1l7wvP1x"
      },
      "outputs": [],
      "source": [
        "all_reviews['sentiment_text']=all_reviews['sentiment'].map({0:'negative',1:'positive'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDGaV1VkO00a"
      },
      "outputs": [],
      "source": [
        "all_reviews = all_reviews.dropna(subset=['review_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aevm8QIKvXYh"
      },
      "outputs": [],
      "source": [
        "#Percentage breakdown of positive, negative reviews.\n",
        "all_reviews['sentiment_text'].value_counts()/len(all_reviews)*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMZBmHt5vt5V"
      },
      "source": [
        "We can observe that our dataset is imbalanced, as indicated by the product data, which contains more positive ratings than negative ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRL8ytITtfK9"
      },
      "outputs": [],
      "source": [
        "#checking duplicates in the data\n",
        "print('Number of duplicated rows: ' , len(all_reviews[all_reviews.duplicated()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCx5Z75Irbdy"
      },
      "outputs": [],
      "source": [
        "data_product['primary_category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0YzXd6GSPim"
      },
      "outputs": [],
      "source": [
        "all_reviews = pd.merge(all_reviews, data_product[['product_id', 'secondary_category']], on='product_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-fCKR56rxMj"
      },
      "outputs": [],
      "source": [
        "#\"Let's examine the percentage of positive and negative ratings within each primary category.\n",
        "all_reviews.groupby('secondary_category')['sentiment_text'].value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9TY7geU0EN3"
      },
      "source": [
        "We can observe that there is class imbalance across all secondary category products."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to resource limitations, specifically in GPUs and memory, I will fine-tune the RoBERTa model only for the Moisturizers and Cleansers categories"
      ],
      "metadata": {
        "id": "hSSw3g-GEr9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kizny_VzS9Ov"
      },
      "outputs": [],
      "source": [
        "interested_categories=['Moisturizers','Cleansers']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKavCu_cTS-Q"
      },
      "outputs": [],
      "source": [
        "all_reviews_2 = all_reviews[all_reviews['secondary_category'].isin(interested_categories)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_icToP-fTYU2"
      },
      "outputs": [],
      "source": [
        "all_reviews_2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37p1_FUGTukK"
      },
      "source": [
        "We can conclude that, overall, customers are satisfied with the products, as reflected in their use of positive words such as 'love,' 'effective,' 'worth,' 'good,' 'holy grail,' 'must-have,' and 'best.' Additionally, it appears that customers are particularly interested in products designed to combat dry skin, such as moisturizers and hydrating formulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EU5gATITtp_"
      },
      "outputs": [],
      "source": [
        "review_text = all_reviews['review_text'].dropna()\n",
        "text = ' '.join(review_text)\n",
        "wordcloud = WordCloud(width = 800, height = 800,\n",
        "                background_color ='white',\n",
        "                stopwords = STOPWORDS,\n",
        "                min_font_size = 10).generate(text)\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeVfmsl-oZdt"
      },
      "source": [
        "Upon examining the detailed comments from customers, we notice that products addressing sensetive skin are frequently mentioned, and the topic of skin is one of the most popular. Additionally, the word 'love' appears quite often indicating product satisfaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOx8KBCIEsqO"
      },
      "source": [
        "Lets look what words are mostly assosiated with good reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRZIUbNKEZye"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "positive_reviews = all_reviews[all_reviews['sentiment'] == 1]\n",
        "review_text_pos = positive_reviews['review_text']\n",
        "text = ' '.join(review_text_pos)\n",
        "wordcloud = WordCloud(width = 800, height = 800,\n",
        "                background_color ='white',\n",
        "                stopwords = STOPWORDS,\n",
        "                min_font_size = 10).generate(text)\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that words such as 'love,' 'amazing,' 'recommend,' 'well,' 'great,' 'gentle,' and others are strongly associated with positive reviews"
      ],
      "metadata": {
        "id": "OI4p7Hl6Wy6I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QEGF1KAFpqq"
      },
      "outputs": [],
      "source": [
        "bad_rev = all_reviews[all_reviews['sentiment'] == 0]\n",
        "review_text = bad_rev['review_text']\n",
        "text = ' '.join(review_text)\n",
        "wordcloud = WordCloud(width = 800, height = 800,\n",
        "                background_color ='white',\n",
        "                stopwords = STOPWORDS,\n",
        "                min_font_size = 10).generate(text)\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that words such as 'price,' 'unfortunately,' 'redness,' and 'dry skin' are commonly associated with negative reviews."
      ],
      "metadata": {
        "id": "umaXOouFXM1S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI5KP2XCcSph"
      },
      "source": [
        "# HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re6_6poDOxKp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#checking for HTMLS\n",
        "import re\n",
        "def find_html(text):\n",
        "    html_pattern = r'<[^>]+>'\n",
        "    return re.findall(html_pattern, text)\n",
        "html_in_reviews = all_reviews['review_text'].apply(find_html)\n",
        "html_in_reviews = html_in_reviews[html_in_reviews.str.len() > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvbCCxM6cTjd"
      },
      "source": [
        "#Upper case words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_WJ7ZvscWLU"
      },
      "source": [
        "Let's examine whether there are any words that are written entirely in uppercase. In sentiment analysis, such words can often indicate strong emotions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SspFO5WLhtFG"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "def is_upper_case(text):\n",
        "  return [word for word in text.split() if word.isupper() and word != 'I']\n",
        "\n",
        "unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbXOW03-J2yw"
      },
      "source": [
        "#URLS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_0qA9sEKsI1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#checking if there are URLS in the data\n",
        "def find_urls(text):\n",
        "    url_pattern = r'(https?://\\S+|www\\.\\S+)'\n",
        "    return re.findall(url_pattern, text)\n",
        "urls_in_reviews = all_reviews['review_text'].apply(find_urls)\n",
        "urls_in_reviews = urls_in_reviews[urls_in_reviews.str.len() > 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_-zfrFQE3A1"
      },
      "outputs": [],
      "source": [
        "sentence = \"Check out this website: https://www.example.com and this one: http://anothersite.net.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "original = tokenizer.convert_tokens_to_string(tokens)\n",
        "print(original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3lb5yDBSO_I"
      },
      "source": [
        "#Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTiLb_q1SObm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def find_at_symbols(text):\n",
        "    at_pattern = r'@'\n",
        "    return re.findall(at_pattern, text)\n",
        "at_symbols_in_reviews = all_reviews['review_text'].apply(find_at_symbols)\n",
        "at_symbols_in_reviews = at_symbols_in_reviews[at_symbols_in_reviews.str.len() > 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypYckuvNFCz7"
      },
      "outputs": [],
      "source": [
        "sentence = \"Hey @username, check this out! Also, @anotheruser might be interested.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "original = tokenizer.convert_tokens_to_string(tokens)\n",
        "print(original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysnYDi4HVFgq"
      },
      "source": [
        "# checking for emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46vuSKdtVH0t",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def find_emojis(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        r'['\n",
        "        u'\\U0001F600-\\U0001F64F'  # Emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # Symbols & Pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # Transport & Map Symbols\n",
        "        u'\\U0001F700-\\U0001F77F'  # Alchemical Symbols\n",
        "        u'\\U0001F780-\\U0001F7FF'  # Geometric Shapes Extended\n",
        "        u'\\U0001F800-\\U0001F8FF'  # Supplemental Arrows-C\n",
        "        u'\\U0001F900-\\U0001F9FF'  # Supplemental Symbols and Pictographs\n",
        "        u'\\U0001FA00-\\U0001FA6F'  # Chess Symbols\n",
        "        u'\\U0001FA70-\\U0001FAFF'  # Symbols and Pictographs Extended-A\n",
        "        u'\\U00002702-\\U000027B0'  # Dingbats\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.findall(text)\n",
        "emojis_in_reviews = all_reviews['review_text'].apply(find_emojis)\n",
        "emojis_in_reviews = emojis_in_reviews[emojis_in_reviews.str.len() > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG4eww8YsuM5"
      },
      "source": [
        "Testing if RoBERTA model can tokenize the emojies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLF1b1W5tk38"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaModel, RobertaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDPOt0rns0Dd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the tokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xBscMKECPu4"
      },
      "outputs": [],
      "source": [
        "a=tokenizer.tokenize(\"Mixed feelings about this 🤔🤷‍♂️\")\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgw5_MYPC_sP"
      },
      "outputs": [],
      "source": [
        "original_sentence = tokenizer.convert_tokens_to_string(a)\n",
        "print(original_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANArBfSRttKt"
      },
      "outputs": [],
      "source": [
        "#LETS TRY ANOTHER SENTENCE WITH HEART TOKENS WHICH ARE RELEVANT FOR THIS TOPIC\n",
        "b=tokenizer.tokenize(\"I love this product ❤️❤️❤️❤️❤️❤️❤️\")\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz6vQpW5t49o"
      },
      "outputs": [],
      "source": [
        "#now lets decode it back to the original sentence\n",
        "original_sentence = tokenizer.convert_tokens_to_string(b)\n",
        "print(original_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpNzrC7aEDEW"
      },
      "source": [
        "The tokenizer performs well, effectively recognizing and tokenizing HTML elements, URLs, emojis, and tags.✌"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWE6bOyVKArn"
      },
      "outputs": [],
      "source": [
        "#does it tokenize tags like @username?\n",
        "c=tokenizer.tokenize(\"@username used this\")\n",
        "print(c)\n",
        "original_sentence = tokenizer.convert_tokens_to_string(c)\n",
        "print(original_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQOJru1lKPxW"
      },
      "outputs": [],
      "source": [
        "#and does it tokenize URL?\n",
        "d=tokenizer.tokenize(\"Check out this website: https://www.example.com and this one: http://anothersite.net.\")\n",
        "print(d)\n",
        "original_sentence = tokenizer.convert_tokens_to_string(d)\n",
        "print(original_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv3RevnvKXmn"
      },
      "outputs": [],
      "source": [
        "#and does it tokenize HTML ?\n",
        "e=tokenizer.tokenize(\"This is a sentence with some <b>bold</b> text and a <a href='https://www.example.com'>link</a>.\")\n",
        "print(e)\n",
        "original_sentence = tokenizer.convert_tokens_to_string(e)\n",
        "print(original_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ0utfo6KgkB"
      },
      "source": [
        "so far looks good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkPWYU2npd4-"
      },
      "source": [
        "## Fine tuning RoBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-CrXIppl2n"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq7U-A3Jy94O"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P8n2S9HqXxL"
      },
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Ak4Ac0qZaY"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i77_dRcuk3z"
      },
      "outputs": [],
      "source": [
        "new_df=all_reviews_2[['review_text','sentiment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnmu1VEhT5qA"
      },
      "outputs": [],
      "source": [
        "new_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8dTcUg3AY0g"
      },
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytdxFWTS-joK"
      },
      "source": [
        "#Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eanaxdihuoyS"
      },
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "TEST_BATCH_SIZE = 8\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLH_JLcxUHJ-"
      },
      "outputs": [],
      "source": [
        "#=precentage the number of posotove negative and neutral reviews\n",
        "new_df['sentiment'].value_counts()/len(new_df)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIPiK-eRUuTL"
      },
      "outputs": [],
      "source": [
        "new_df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxAfXIuBzlNS"
      },
      "outputs": [],
      "source": [
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.review_text\n",
        "        self.targets = self.data.sentiment\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),#1 if masked 0 if not\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)#The sentiment label for this text.\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRfHoSwg2oFJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First, split into train+validation and test sets\n",
        "train_val_size = 0.8\n",
        "train_val_data = new_df.sample(frac=train_val_size, random_state=200)\n",
        "test_data = new_df.drop(train_val_data.index).reset_index(drop=True)\n",
        "\n",
        "# Now split train_val_data into train and validation sets\n",
        "# Validation set will be 10% of train_val_data, which is 12.5% of the original train set (0.1 / 0.8 = 0.125)\n",
        "train_data, val_data = train_test_split(train_val_data, test_size=0.125, random_state=200)\n",
        "\n",
        "# Reset indices\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "test_data = test_data.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"VALIDATION Dataset: {}\".format(val_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
        "\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "validation_set = SentimentData(val_data, tokenizer, MAX_LEN)\n",
        "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEGSCzAA6mIR"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "Validation_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "validation_loader = DataLoader(validation_set, **Validation_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwSWyPmS-mkF"
      },
      "source": [
        "#Creating the Neural Network for Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4euzifK-gam"
      },
      "outputs": [],
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK0Sv10kRWv_"
      },
      "outputs": [],
      "source": [
        "model = RobertaClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRsYHlGM9sO2"
      },
      "outputs": [],
      "source": [
        "count_clases=new_df['sentiment'].value_counts()\n",
        "count_clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmC1C6Hq-AqR"
      },
      "outputs": [],
      "source": [
        "class_weights = np.array(count_clases/np.sum(count_clases))\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBeTHcby-nO8"
      },
      "outputs": [],
      "source": [
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsWd3amRRmqw"
      },
      "outputs": [],
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp2kQ_SxY_Q4"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLBz70OQY5B7"
      },
      "outputs": [],
      "source": [
        "#After every 2500 steps the loss value is printed in the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D5W0eZV_p9s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "def train_and_validate(epochs, model, training_loader, validation_loader, loss_function, optimizer, device, patience=2):\n",
        "    model_save_path = '/content/drive/MyDrive/best_model.pt'\n",
        "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        tr_loss = 0\n",
        "        all_tr_targets = []\n",
        "        all_tr_outputs = []\n",
        "        nb_tr_steps = 0\n",
        "\n",
        "        for _, data in tqdm(enumerate(training_loader, 0), desc=f\"Training Epoch {epoch+1}\"):\n",
        "            ids = data['ids'].to(device, dtype=torch.long)\n",
        "            mask = data['mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype=torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            all_tr_targets.extend(targets.cpu().detach().numpy())\n",
        "            all_tr_outputs.extend(big_idx.cpu().detach().numpy())\n",
        "            nb_tr_steps += 1\n",
        "            if _ % 2500 == 0:\n",
        "                loss_step = tr_loss / nb_tr_steps\n",
        "                print(f\"Training Loss per 2500 steps: {loss_step}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        epoch_loss = tr_loss / nb_tr_steps\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        tr_f1_scores = f1_score(all_tr_targets, all_tr_outputs, average=None)\n",
        "        tr_macro_f1 = np.mean(tr_f1_scores)\n",
        "        print(f\"Training Loss Epoch {epoch+1}: {epoch_loss}\")\n",
        "        print(f\"Training F1 Scores: {tr_f1_scores}\")\n",
        "        print(f\"Training Macro F1 Score: {tr_macro_f1}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_targets = []\n",
        "        all_val_outputs = []\n",
        "        nb_val_steps = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _, data in tqdm(enumerate(validation_loader, 0), desc=f\"Validation Epoch {epoch+1}\"):\n",
        "                ids = data['ids'].to(device, dtype=torch.long)\n",
        "                mask = data['mask'].to(device, dtype=torch.long)\n",
        "                token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "                targets = data['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "                outputs = model(ids, mask, token_type_ids)\n",
        "                loss = loss_function(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "                all_val_targets.extend(targets.cpu().detach().numpy())\n",
        "                all_val_outputs.extend(big_idx.cpu().detach().numpy())\n",
        "\n",
        "                nb_val_steps += 1\n",
        "\n",
        "        val_epoch_loss = val_loss / nb_val_steps\n",
        "        val_losses.append(val_epoch_loss)\n",
        "\n",
        "        val_f1_scores = f1_score(all_val_targets, all_val_outputs, average=None)\n",
        "        val_macro_f1 = np.mean(val_f1_scores)\n",
        "        print(f\"Validation Loss Epoch {epoch+1}: {val_epoch_loss}\")\n",
        "        print(f\"Validation F1 Scores: {val_f1_scores}\")\n",
        "        print(f\"Validation Macro F1 Score: {val_macro_f1}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_epoch_loss < best_val_loss:\n",
        "            best_val_loss = val_epoch_loss\n",
        "            patience_counter = 0  # Reset the patience counter if loss improves\n",
        "            torch.save(model.state_dict(), model_save_path)  # Save best model to Google Drive\n",
        "            print(f\"Best model saved with Validation Loss: {best_val_loss}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss per Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Training and Validation completed!\")\n",
        "\n",
        "# Usage\n",
        "epochs = 5\n",
        "train_and_validate(epochs, model, training_loader, validation_loader, loss_function, optimizer, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The early stopping rule was triggered at epoch 3, indicating that further training would likely result in overfitting. Additionally, the validation loss is lower than the training loss, which may be due to the validation samples being drawn from the training set, resulting in high similarity between the two."
      ],
      "metadata": {
        "id": "keEM2sU3QYXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T2eLpEtYH4g",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/best_model.pt'))\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test data\n",
        "def test_my_model(model, testing_loader,device):\n",
        "    model.eval()\n",
        "    all_test_targets = []\n",
        "    all_test_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            big_val, big_idx = torch.max(outputs, dim=1)\n",
        "            all_test_targets.extend(targets.cpu().detach().numpy())\n",
        "            all_test_outputs.extend(big_idx.cpu().detach().numpy())\n",
        "    test_f1_scores = f1_score(all_test_targets, all_test_outputs, average=None)\n",
        "    test_macro_f1 = np.mean(test_f1_scores)\n",
        "    print(f\"test F1 Scores: {test_f1_scores}\")\n",
        "    print(f\"test Macro F1 Score: {test_macro_f1}\")\n",
        "test_my_model(model, testing_loader,device)"
      ],
      "metadata": {
        "id": "Cf8tlUZMkLCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying our model to the test dataset, we observed high F1 scores for both the positive and negative classes, indicating strong performance in balancing precision and recall. This means the model effectively identifies true positives while minimizing false positives and negatives. In our case, with an imbalanced dataset, the F1 score is especially valuable as it offers a more insightful evaluation than accuracy alone, showing the model’s ability to distinguish between sentiment classes with optimized precision and recall."
      ],
      "metadata": {
        "id": "DQsK8rinRVE_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}