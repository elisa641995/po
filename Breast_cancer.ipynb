{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisa641995/po/blob/main/Breast_cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K96Gc52dRt78"
      },
      "source": [
        "##imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NALRFLhMRyiB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import cv2\n",
        "from torchvision.ops import box_iou\n",
        "from torchvision.transforms import v2\n",
        "from google.colab.patches import cv2_imshow  # For displaying images in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPI8oahtzDXS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgrYNMtIzD8X"
      },
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FRMyfNESwKw"
      },
      "outputs": [],
      "source": [
        "# Define the base directory where your folders are located\n",
        "base_dir = '/content/drive/MyDrive/Dataset_BUSI_with_GT'  # Update with the path in your Google Drive\n",
        "\n",
        "# Define folders and their corresponding labels\n",
        "folders = {'normal': 'normal', 'malignant': 'malignant', 'benign': 'benign'}\n",
        "\n",
        "# Initialize an empty list to store file paths and labels\n",
        "data = []\n",
        "\n",
        "# Loop through each folder and process images\n",
        "for folder_name, label in folders.items():\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "\n",
        "    # Traverse the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if '_mask' not in filename:  # Ignore files with '_mask' in the name\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            data.append({'file_path': file_path, 'label': label})\n",
        "\n",
        "# Create a DataFrame to keep track of file paths and labels\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n",
        "\n",
        "# Save the DataFrame to a CSV file if needed\n",
        "df.to_csv('breast_images_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-VsZLuQVw8G"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I_rqsrnQWD0i"
      },
      "outputs": [],
      "source": [
        "sample_df = df.sample(n=20, random_state=42)\n",
        "plt.figure(figsize=(15, 15))\n",
        "columns = 5\n",
        "for i, row in enumerate(sample_df.iterrows()):\n",
        "    file_path = row[1]['file_path']\n",
        "    label = row[1]['label']\n",
        "    image = cv2.imread(file_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(4, columns, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(label)\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYn-o69dJHWF"
      },
      "source": [
        "##Building clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdK21y8XH9Oa"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images: list, labels: list, transform=None):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlTdsQ1tGMCP"
      },
      "source": [
        "#se block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBAvi9yOCmud"
      },
      "outputs": [],
      "source": [
        "class SE_Block(nn.Module):\n",
        "    def __init__(self, c, r=16):\n",
        "        super(SE_Block, self).__init__()\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(c, c // r, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(c // r, c, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, c, _, _ = x.size()\n",
        "        y = self.squeeze(x).view(bs, c)\n",
        "        y = self.excitation(y).view(bs, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gvafq3CnDVd"
      },
      "source": [
        "#Train test split and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVxYZVKUbhx9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "img_train = T.Compose(\n",
        "    [\n",
        "        T.Resize([250,250]),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomVerticalFlip(),\n",
        "        T.RandomRotation(10),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ],)\n",
        "\n",
        "img_val = T.Compose(\n",
        "    [\n",
        "        T.Resize([250,250]),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "    ],\n",
        ")\n",
        "\n",
        "img_test = T.Compose(\n",
        "    [\n",
        "        T.Resize([250,250]),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDzM7gdhCqbK"
      },
      "outputs": [],
      "source": [
        "#maping the lables  in df\n",
        "df['label'] = df['label'].map({'normal': 0, 'malignant': 1, 'benign': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gSbrLyZb2jZ"
      },
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)  # 80% train, 20% temp\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)  # 50% val, 50% test\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
        "print(\"VALIDATION Dataset: {}\".format(val_df.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
        "train_df_2=Dataset(images=train_df['file_path'].values,labels=train_df['label'].values,transform=img_train)\n",
        "val_df_2=Dataset(images=val_df['file_path'].values,labels=val_df['label'].values,transform=img_val)\n",
        "test_df_2=Dataset(images=test_df['file_path'].values,labels=test_df['label'].values,transform=img_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTBcXfyjg1v_"
      },
      "outputs": [],
      "source": [
        "#hyperparameters\n",
        "BATCH_SIZE = 2\n",
        "epochs = 50\n",
        "LEARNING_RATE = 1e-3                 #learning rate\n",
        "gradient_clip = 0.1                 #gradient clipping\n",
        "weight_decay = 1e-4               #weight decay\n",
        "patience = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJj4mu2-gsBF"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "                'pin_memory':True\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "               'pin_memory':True\n",
        "                }\n",
        "Validation_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "                     'pin_memory':True\n",
        "                }\n",
        "training_loader = DataLoader(train_df_2, **train_params)\n",
        "testing_loader = DataLoader(test_df_2, **test_params)\n",
        "validation_loader = DataLoader(val_df_2, **Validation_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVgJNP5PniqM"
      },
      "source": [
        "#CNN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o41xND4C6B9E"
      },
      "source": [
        "Using 4 convolutional layers strikes a balance between model complexity and data limitations. Medical images have subtle details that a shallow model might miss, while 4 layers allow for effective feature extraction without overfitting on a small dataset of 780 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcILzTtZnkpV"
      },
      "outputs": [],
      "source": [
        "class BreastUltrasoundCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BreastUltrasoundCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(28800, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        self.fc4 = nn.Linear(64, 3)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.leaky_relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 28800)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x= self.dropout(F.relu(self.fc3(x)))\n",
        "        x = F.softmax(self.fc4(x), dim=1)\n",
        "        return x # matrix of probabilitis of batch size*num of clases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh7R48X6H9pf"
      },
      "source": [
        "#CNN model with se block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE4U3FgPILg7"
      },
      "outputs": [],
      "source": [
        "class BreastUltrasoundCNN_SE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BreastUltrasoundCNN_SE, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.se=SE_Block(16)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.se2=SE_Block(32)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.se3=SE_Block(64)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1)\n",
        "        self.se4=SE_Block(128)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(28800, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        self.fc4 = nn.Linear(64, 3)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    def forward(self,x):\n",
        "      x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n",
        "      x = self.se(x)\n",
        "\n",
        "      x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n",
        "      x = self.se2(x)\n",
        "\n",
        "      x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "      x = self.se3(x)\n",
        "      x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "      x = self.se4(x)\n",
        "      x = x.view(-1, 28800)\n",
        "      x = self.dropout(F.relu(self.fc1(x)))\n",
        "      x = self.dropout(F.relu(self.fc2(x)))\n",
        "      x= self.dropout(F.relu(self.fc3(x)))\n",
        "      x = F.softmax(self.fc4(x), dim=1)\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vstIg4Of6DEw"
      },
      "outputs": [],
      "source": [
        "my_model_cnn = BreastUltrasoundCNN()\n",
        "my_model_cnn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dz_Ovo6G2Pm"
      },
      "outputs": [],
      "source": [
        "count_clases=df['label'].value_counts()\n",
        "count_clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWM2FLptGtKc"
      },
      "outputs": [],
      "source": [
        "class_weights = np.array(count_clases/np.sum(count_clases))\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flr1RsEXG_Yp"
      },
      "outputs": [],
      "source": [
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(params =my_model_cnn.parameters(), lr=LEARNING_RATE,weight_decay=1e-3 )\n",
        "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.0001, epochs=50, steps_per_epoch=len(training_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtEpt2HhIp4S"
      },
      "source": [
        "\n",
        "\n",
        "#Training base cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-eD8m7WKuq"
      },
      "source": [
        "##Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rKG-X-Z6o26"
      },
      "outputs": [],
      "source": [
        "def my_train(model, train_loader, val_loader, optimizer, loss_function, scheduler, epochs, patience=2, device=\"cuda\"):\n",
        "    #model_save_path = '/content/drive/MyDrive/best_model_breast_cnn.pt'\n",
        "    #os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        step = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for step, (images, labels) in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            prediction = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(prediction.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            loss = loss_function(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            step += 1\n",
        "            if step % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Step {step}, Loss: {train_loss/step:.4f},LR:{scheduler.get_last_lr()}\")\n",
        "        # Compute F1 scores for training\n",
        "        unique_labels = np.unique(all_labels)\n",
        "        tr_f1_scores = f1_score(all_preds, all_labels, average=None, labels=unique_labels)\n",
        "        tr_f1_score_dict = {label: score for label, score in zip(unique_labels, tr_f1_scores)}\n",
        "        tr_macro_f1 = f1_score(all_preds, all_labels, average='macro', labels=unique_labels)\n",
        "        train_loss_epoch = train_loss / step\n",
        "        training_losses.append(train_loss_epoch)\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        nb_val_steps = 0\n",
        "        all_preds_v, all_labels_v = [], []\n",
        "        with torch.no_grad():\n",
        "            for val_step, (val_images, val_labels) in enumerate(val_loader):\n",
        "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
        "                outputs_v = model(val_images)\n",
        "                loss_v = loss_function(outputs_v, val_labels)\n",
        "                val_loss += loss_v.item()\n",
        "                nb_val_steps += 1\n",
        "                prediction_v = torch.argmax(outputs_v, dim=1)\n",
        "                all_preds_v.extend(prediction_v.cpu().numpy())\n",
        "                all_labels_v.extend(val_labels.cpu().numpy())\n",
        "\n",
        "        # Compute F1 scores for validation\n",
        "        val_loss_epoch = val_loss / nb_val_steps\n",
        "        validation_losses.append(val_loss_epoch)\n",
        "        unique_labels_v = np.unique(all_labels_v)\n",
        "        val_f1_scores = f1_score(all_preds_v, all_labels_v, average=None, labels=unique_labels_v)\n",
        "        val_f1_score_dict = {label: score for label, score in zip(unique_labels_v, val_f1_scores)}\n",
        "        val_macro_f1 = f1_score(all_preds_v, all_labels_v, average='macro', labels=unique_labels_v)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss/step:.4f}, Validation Loss: {val_loss_epoch:.4f}\")\n",
        "        print(f\"Training F1 Scores: {tr_f1_score_dict}, Validation F1 Scores: {val_f1_score_dict}\")\n",
        "        print(f\"Training Macro F1: {tr_macro_f1:.4f}, Validation Macro F1: {val_macro_f1:.4f}\")\n",
        "\n",
        "        if val_loss_epoch < best_val_loss:\n",
        "            best_val_loss = val_loss_epoch\n",
        "            #torch.save(model.state_dict(), model_save_path)\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "        if no_improvement_count >= patience:\n",
        "            print(\"Early stopping triggered. No improvement in validation loss for {} epochs.\".format(patience))\n",
        "            break\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(training_losses) + 1), training_losses, label=\"Training Loss\")\n",
        "    plt.plot(range(1, len(validation_losses) + 1), validation_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss per Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    print(\"Training and Validation completed!\")\n",
        "\n",
        "#my_train(my_model, training_loader, validation_loader, optimizer, loss_function, sched, epochs, patience=patience, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4caiUWtLZnV"
      },
      "outputs": [],
      "source": [
        "my_train(my_model_cnn, training_loader, validation_loader, optimizer, loss_function, sched, epochs, patience=patience, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifc4hZo1WO3s"
      },
      "source": [
        "##Test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVBM8J6xN5Tp"
      },
      "outputs": [],
      "source": [
        "#test set function\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    nb_test_steps = 0\n",
        "    all_preds_s, all_labels_s = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "            for test_step, (test_images, test_labels) in enumerate(test_loader):\n",
        "                test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
        "                outputs_s = model(test_images)\n",
        "                prediction_s = torch.argmax(outputs_s, dim=1)\n",
        "                all_preds_s.extend(prediction_s.cpu().numpy())\n",
        "                all_labels_s.extend(test_labels.cpu().numpy())\n",
        "    unique_labels_s = np.unique(all_labels_s)\n",
        "    test_f1_scores = f1_score(all_preds_s, all_labels_s, average=None, labels=unique_labels_s)\n",
        "    test_f1_score_dict = {label: score for label, score in zip(unique_labels_s, test_f1_scores)}\n",
        "    test_macro_f1 = f1_score(all_preds_s, all_labels_s, average='macro', labels=unique_labels_s)\n",
        "    print(f\"Test F1 Scores: {test_f1_score_dict}\")\n",
        "    print(f\"Test Macro F1: {test_macro_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-eM6xpWs7Zl"
      },
      "outputs": [],
      "source": [
        "test_model(my_model_cnn, testing_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8vyrGktMhL"
      },
      "source": [
        "#Training CNN with SE block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_uOuqZofljj"
      },
      "outputs": [],
      "source": [
        "cnn_se=BreastUltrasoundCNN_SE()\n",
        "cnn_se.to(device)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(params =cnn_se.parameters(), lr=LEARNING_RATE,weight_decay=1e-3 )\n",
        "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.0001, epochs=50, steps_per_epoch=len(training_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDOC8tNstZE-"
      },
      "outputs": [],
      "source": [
        "\n",
        "#training :\n",
        "my_train(cnn_se, training_loader, validation_loader, optimizer, loss_function, sched, epochs, patience=patience, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPbzALN6t099"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "test_model(cnn_se, testing_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ52VtDQT521"
      },
      "source": [
        "#RESNET50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ajJTjZIkT9Fg"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50\n",
        "model_my = resnet50(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI9fIOtldaHP"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import ResNet50_Weights\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "img_train_res=T.Compose([T.RandomHorizontalFlip(),\n",
        "        T.RandomVerticalFlip(),preprocess])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp9wcBpGWUQ3"
      },
      "outputs": [],
      "source": [
        "class ResNet50Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResNet50Model, self).__init__()\n",
        "    self.resnet =resnet50(pretrained=True)\n",
        "    self.dropout = torch.nn.Dropout(0.3)\n",
        "    self.fc1=torch.nn.Linear(1000,3)\n",
        "  def forward(self,x):\n",
        "    x=self.resnet(x)\n",
        "    x=F.relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x = F.softmax(self.fc1(x), dim=1)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gPiLZl_ltudR"
      },
      "outputs": [],
      "source": [
        "my_resnet=ResNet50Model()\n",
        "my_resnet.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UHhduPr4Fj_"
      },
      "outputs": [],
      "source": [
        "transform_my = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoahK0bPtrKi"
      },
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)  # 80% train, 20% temp\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)  # 50% val, 50% test\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
        "print(\"VALIDATION Dataset: {}\".format(val_df.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
        "train_df_2=Dataset(images=train_df['file_path'].values,labels=train_df['label'].values,transform=img_train_res)\n",
        "val_df_2=Dataset(images=val_df['file_path'].values,labels=val_df['label'].values,transform=preprocess)\n",
        "test_df_2=Dataset(images=test_df['file_path'].values,labels=test_df['label'].values,transform=preprocess)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J_hQ9eLt9tt"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "                'pin_memory':True\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "               'pin_memory':True\n",
        "                }\n",
        "Validation_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "                     'pin_memory':True\n",
        "                }\n",
        "training_loader = DataLoader(train_df_2, **train_params)\n",
        "testing_loader = DataLoader(test_df_2, **test_params)\n",
        "validation_loader = DataLoader(val_df_2, **Validation_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rtfueLCuBWy"
      },
      "outputs": [],
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(params =my_resnet.parameters(), lr=LEARNING_RATE,weight_decay=1e-3 )\n",
        "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0001, epochs=50, steps_per_epoch=len(training_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZkN-AwauLjd"
      },
      "outputs": [],
      "source": [
        "my_train(my_resnet, training_loader, validation_loader, optimizer, loss_function, sched, epochs, patience=2, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81GytY66vBR7"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "test_model(my_resnet, testing_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-RLSpIuirX"
      },
      "source": [
        "#VGG19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wWoGWLTGulD4"
      },
      "outputs": [],
      "source": [
        "model = models.vgg19(pretrained=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UPldC0HpwcHN"
      },
      "outputs": [],
      "source": [
        "class my_vgg19(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(my_vgg19, self).__init__()\n",
        "    self.vgg =model\n",
        "    self.dropout = torch.nn.Dropout(0.3)\n",
        "    self.fc1=nn.Linear(1000,3)\n",
        "  def forward(self,x):\n",
        "    x=self.vgg(x)\n",
        "    x=F.relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x = F.softmax(self.fc1(x), dim=1)\n",
        "    return x\n",
        "my_vgg=my_vgg19()\n",
        "my_vgg.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOOB59M2wzFJ"
      },
      "outputs": [],
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(params =my_vgg.parameters(), lr=LEARNING_RATE,weight_decay=1e-3 )\n",
        "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.00001, epochs=50, steps_per_epoch=len(training_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iDBuhUSxjiy"
      },
      "outputs": [],
      "source": [
        "#training\n",
        "my_train(my_vgg, training_loader, validation_loader, optimizer, loss_function, sched, epochs, patience=1, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39au9OmN4FNb"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "test_model(my_vgg, testing_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RoNbTqWeB4"
      },
      "source": [
        "#Results of classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ELGDJ0ZD1f"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    'CNN': [0, 0.5625, 0.7105263157894737, 0.4243],\n",
        "    'CNN SE BLOCK': [0, 0.717948717948718, 0.7920792079207921, 0.5033],\n",
        "    'RESNET50': [0.8, 0.5714285714285714, 0.5714285714285714, 0.7429],\n",
        "    'VGG19': [0.6829268292682927, 0.7692307692307693, 0.8571428571428571, 0.7453]\n",
        "}\n",
        "\n",
        "\n",
        "index = ['normal_f1_score', 'malignant_f1_score', 'benign_f1_score', 'macro_f1_score']\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(data, index=index)\n",
        "\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joc7b0n0uYFw"
      },
      "source": [
        "After experimenting with various models, we observe that simple architectures like a basic CNN and an upgraded CNN with an SE block fail to deliver satisfactory results. This is primarily due to the limited amount of training data, making it challenging for these models to learn effectively.\n",
        "\n",
        "However, when transitioning to more complex and deeper models, particularly pretrained ones, there is a noticeable improvement in performance. Among these, VGG19 stands out as the best-performing model. While its results are not perfect, it demonstrates significant improvement in classification abilities, as reflected in the F1 scores for each class, including the under represented classes \"normal\" and \"malignant.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaSkvX9mg8qn"
      },
      "source": [
        "#Breast Cancer Image Segmentation | Attention UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzHfclyINO5X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Dataset_BUSI_with_GT'\n",
        "folders = {'normal': 'normal', 'malignant': 'malignant', 'benign': 'benign'}\n",
        "\n",
        "def get_base_filename(filename):\n",
        "    return re.sub(r'_mask(_\\d+)?\\.png$', '.png', filename)\n",
        "\n",
        "def process_images():\n",
        "    image_paths = []\n",
        "    mask_paths = []\n",
        "\n",
        "    for folder_name, label in folders.items():\n",
        "        folder_path = os.path.join(base_dir, folder_name)\n",
        "        files = os.listdir(folder_path)\n",
        "\n",
        "        # Group files by their base name\n",
        "        file_groups = {}\n",
        "        for file in files:\n",
        "            base_name = get_base_filename(file)\n",
        "            if base_name not in file_groups:\n",
        "                file_groups[base_name] = []\n",
        "            file_groups[base_name].append(file)\n",
        "\n",
        "        # Process each group\n",
        "        for base_name, group in file_groups.items():\n",
        "            if len(group) == 2:  # One original image and one mask\n",
        "                original_image = next(f for f in group if not f.endswith('_mask.png'))\n",
        "                mask_image = next(f for f in group if f.endswith('_mask.png'))\n",
        "\n",
        "                image_paths.append(os.path.join(folder_path, original_image))\n",
        "                mask_paths.append(os.path.join(folder_path, mask_image))\n",
        "            elif len(group) > 2:  # More than one mask file\n",
        "                print(f\"Removing group due to multiple masks: {group}\")\n",
        "                for file in group:\n",
        "                    file_path = os.path.join(folder_path, file)\n",
        "                    try:\n",
        "                        os.remove(file_path)\n",
        "                        print(f\"Deleted: {file_path}\")\n",
        "                    except OSError as e:\n",
        "                        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "# Process images and create DataFrame\n",
        "image_paths, mask_paths = process_images()\n",
        "\n",
        "df_segmentation = pd.DataFrame({\n",
        "    'image_path': image_paths,\n",
        "    'mask_path': mask_paths\n",
        "})\n",
        "\n",
        "df_segmentation.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oWmd9qg06E1"
      },
      "outputs": [],
      "source": [
        "df_segmentation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebhfNzmcjVpZ"
      },
      "outputs": [],
      "source": [
        "#Creating Dataset class for the segmentaiton task\n",
        "class Dataset_seg(torch.utils.data.Dataset):\n",
        "    def __init__(self, images: list, masked_image: list, transform=None):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.masked_image = masked_image\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('L')\n",
        "        masked_img = Image.open(self.masked_image[idx]).convert('L')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            masked_img = self.transform(masked_img)\n",
        "        print(image.shape,masked_img.shape)\n",
        "        return image, masked_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8lfxVrRSB1b"
      },
      "source": [
        "##The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgZ5NZuQhBoO"
      },
      "outputs": [],
      "source": [
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv=nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "      x=self.conv(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpWlt224jnXY"
      },
      "outputs": [],
      "source": [
        "class my_encoder(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv=conv_block(in_c, out_c)\n",
        "        self.pool=nn.MaxPool2d((2, 2))\n",
        "    def forward(self, x):\n",
        "      s=self.conv(x)# s for the skip connection\n",
        "      p=self.pool(s)\n",
        "      #print(s,p)\n",
        "      return s,p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvrr_qqIN7rn"
      },
      "outputs": [],
      "source": [
        "class my_attention_gate(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Wg = nn.Sequential(\n",
        "            nn.Conv2d(in_c[0], out_c, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(out_c)\n",
        "        )\n",
        "        self.Ws = nn.Sequential(\n",
        "            nn.Conv2d(in_c[1], out_c, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(out_c)\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(out_c, out_c, kernel_size=1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, g, s):\n",
        "        Wg = self.Wg(g)\n",
        "        Ws = self.Ws(s)\n",
        "        out = self.relu(Wg + Ws)\n",
        "        out = self.output(out)\n",
        "        return out * s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6MPpneIXGjZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class my_decoder(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.ag = my_attention_gate(in_c, out_c)\n",
        "        self.c1 = conv_block(in_c[0]+out_c, out_c)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.up(x)\n",
        "        s = self.ag(x, s)\n",
        "        x = torch.cat([x, s], axis=1)\n",
        "        x = self.c1(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMPIbtuCldA0"
      },
      "outputs": [],
      "source": [
        "class attention_unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encodeer_1=my_encoder(1,64)\n",
        "        self.encodeer_2=my_encoder(64,128)\n",
        "        self.encodeer_3=my_encoder(128,256)\n",
        "        self.encodeer_4=my_encoder(256,512)\n",
        "        self.conv1=conv_block(512,1024)\n",
        "        self.decodeer_1=my_decoder([1024,512],512)\n",
        "        self.decodeer_2=my_decoder([512,256],256)\n",
        "        self.decodeer_3=my_decoder([256,128],128)\n",
        "        self.decodeer_4=my_decoder([128,64],64)\n",
        "        self.conv2=nn.Conv2d(64,1 , kernel_size=1,padding=0)\n",
        "        self.seg=nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "      s1,p1=self.encodeer_1(x)#s_size[batchsize,64,256,256],psize[batchsize,64,128,128]\n",
        "      #print(s1.shape,p1.shape)\n",
        "      s2,p2=self.encodeer_2(p1)#s_size[batchsize,128,128,128],psize[batchsize,128,64,64]\n",
        "      s3,p3=self.encodeer_3(p2)\n",
        "      s4,p4=self.encodeer_4(p3)\n",
        "      c1=self.conv1(p4)\n",
        "      d1=self.decodeer_1(c1,s4)\n",
        "      d2=self.decodeer_2(d1,s3)\n",
        "      d3=self.decodeer_3(d2,s2)\n",
        "      d4=self.decodeer_4(d3,s1)\n",
        "      output=self.conv2(d4)\n",
        "      #softmax\n",
        "      output = self.seg(output)\n",
        "      return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbsAtz45MTKX"
      },
      "outputs": [],
      "source": [
        "model=attention_unet()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwYIVo7Gjlwx"
      },
      "outputs": [],
      "source": [
        "#Data transform\n",
        "transform_my_train = T.Compose([\n",
        "    T.Resize([256,256]),\n",
        "    T.RandomHorizontalFlip(),\n",
        "        T.RandomVerticalFlip(),\n",
        "    T.ToTensor()])\n",
        "transform_test_val=T.Compose([\n",
        "    T.Resize([256,256]),\n",
        "    T.ToTensor()])\n",
        "train_df, temp_df = train_test_split(df_segmentation, test_size=0.2, random_state=42)  # 80% train, 20% temp\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 50% val, 50% test\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
        "print(\"VALIDATION Dataset: {}\".format(val_df.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
        "train_df_2=Dataset_seg(images=train_df['image_path'].values,masked_image=train_df['mask_path'].values,transform=transform_test_val)\n",
        "val_df_2=Dataset_seg(images=val_df['image_path'].values,masked_image=val_df['mask_path'].values,transform=transform_test_val)\n",
        "test_df_2=Dataset_seg(images=test_df['image_path'].values,masked_image=test_df['mask_path'].values,transform=transform_test_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63leazfuofkb"
      },
      "outputs": [],
      "source": [
        "orginal_image,mask_image=test_df_2[1]\n",
        "orginal_image.shape,mask_image.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZTP0VJnpwsj"
      },
      "outputs": [],
      "source": [
        "image_np = orginal_image.squeeze().cpu().numpy()\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(image_np, cmap='gray')  # Use 'gray' colormap for grayscale images\n",
        "plt.axis('off')  # Optional: hide axis\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3SdaM9osSKC"
      },
      "outputs": [],
      "source": [
        "#training loader\n",
        "\n",
        "train_params = {'batch_size': 4,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "                'pin_memory':True\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 4,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "               'pin_memory':True\n",
        "                }\n",
        "Validation_params = {'batch_size': 4,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "                     'pin_memory':True\n",
        "                }\n",
        "training_loader = DataLoader(train_df_2, **train_params)\n",
        "testing_loader = DataLoader(test_df_2, **test_params)\n",
        "validation_loader = DataLoader(val_df_2, **Validation_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnlbtCvQJR9W"
      },
      "outputs": [],
      "source": [
        "#dice loss\n",
        "def dice_loss(pred, target, smooth = 1e-6):\n",
        "    pred = pred.contiguous()\n",
        "    target = target.contiguous()\n",
        "    intersection=2*(pred*target).sum()+smooth\n",
        "    dominator_sum=pred.sum()+target.sum()+smooth\n",
        "    dice_loss=1-((intersection+smooth)/(dominator_sum+smooth))\n",
        "    return dice_loss\n",
        "def bce_dice_loss(pred, target):\n",
        "    bce = torch.nn.BCELoss()\n",
        "    bce_loss = bce(pred, target)\n",
        "    dice = dice_loss(pred, target)\n",
        "    return bce_loss + dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cML-KOnLSHkk"
      },
      "outputs": [],
      "source": [
        "#IUO matric function\n",
        "def iou_score(output, target):\n",
        "    smooth = 1e-5\n",
        "    output = output.type(torch.LongTensor)\n",
        "    target = target.type(torch.LongTensor)\n",
        "    intersection=(output & target).float().sum((1, 2))\n",
        "    union=(output | target).float().sum((1, 2))\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    thresholded = torch.clamp(20 * (iou - 0.55), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
        "    return thresholded.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r4DZybbMCfc"
      },
      "outputs": [],
      "source": [
        "#optimizer\n",
        "optimizer = torch.optim.Adam(params =model.parameters(), lr=0.0001,weight_decay=1e-3 )\n",
        "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.001, epochs=50, steps_per_epoch=len(training_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45juShXaVgGv"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlQ-95F0OxI8"
      },
      "outputs": [],
      "source": [
        "#training function for attention unet\n",
        "def train_attention_unet(model,optimizer,loss_function,train_loader,val_loader,epochs,device,patience,scheduler):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    iou_scores_train=[]\n",
        "    iou_scores_val=[]\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        step = 0\n",
        "        iou_train=0\n",
        "        all_preds, all_labels = [], []\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for step, (images,masked_image ) in progress_bar:\n",
        "            images, masked_image = images.to(device), masked_image.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs=model(images)\n",
        "            threshold = 0.55\n",
        "            prediction = (outputs > threshold)\n",
        "            loss=loss_function(outputs,masked_image)\n",
        "            iou_my=iou_score(prediction, masked_image)\n",
        "            #convert to integer\n",
        "\n",
        "            iou_my =iou_my.cpu().numpy()\n",
        "            iou_train+=iou_my\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sched.step()\n",
        "\n",
        "            step += 1\n",
        "            if step % 20 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Step {step}, Loss: {train_loss/step:.4f}, IOU: {iou_train/step},LR:{scheduler.get_last_lr()}\")\n",
        "        #validation loss\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        iou_val=0\n",
        "        nb_val_steps = 0\n",
        "        all_preds_v, all_labels_v = [], []\n",
        "        with torch.no_grad():\n",
        "          for val_step, (val_image, val_mask) in enumerate(val_loader):\n",
        "            val_images, val_masks = val_image.to(device), val_mask.to(device)\n",
        "            outputs_v = model(val_images)\n",
        "            prediction_v = (outputs_v > 0.55)\n",
        "            loss_v = loss_function(outputs_v, val_masks)\n",
        "            val_loss += loss_v.item()\n",
        "            nb_val_steps += 1\n",
        "            iou_my_v=iou_score(prediction_v, val_masks)\n",
        "            iou_my_v=iou_my_v.cpu().numpy()\n",
        "            iou_val+=iou_my_v\n",
        "\n",
        "        train_loss_epoch = train_loss/step\n",
        "        val_loss_epoch = val_loss / nb_val_steps\n",
        "        iou_train_epoch=iou_train/step\n",
        "        iou_val_epoch=iou_val/nb_val_steps\n",
        "        training_losses.append(train_loss_epoch)\n",
        "        validation_losses.append(val_loss_epoch)\n",
        "        iou_scores_train.append(iou_train_epoch)\n",
        "        iou_scores_val.append(iou_val_epoch)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss_epoch:.4f}, Validation Loss: {val_loss_epoch}\")\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training IOU: {iou_train_epoch:.4f}, Validation IOU: {iou_val_epoch}\")\n",
        "        if val_loss_epoch < best_val_loss:\n",
        "            best_val_loss = val_loss_epoch\n",
        "            print(f\"Best validation loss {best_val_loss}\")\n",
        "            #torch.save(model.state_dict(), model_save_path)\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= patience:\n",
        "              print(\"Early stopping triggered. No improvement in validation loss for {} epochs.\".format(patience))\n",
        "              break\n",
        "    plt.figure(figsize=(20,8))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.plot(training_losses, label=\"Training\")\n",
        "    plt.plot(validation_losses, label=\"Validtion\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title(\"Model IoU\")\n",
        "    plt.plot(iou_scores_train, label=\"Training\")\n",
        "    plt.plot(iou_scores_val, label=\"Validtion\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6NNywp3lyNy"
      },
      "outputs": [],
      "source": [
        "#training\n",
        "train_attention_unet(model,optimizer,bce_dice_loss,training_loader,validation_loader,50,device,3,sched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bC61wrmve7e"
      },
      "source": [
        "The results on the validation data outperform those on the training data in terms of IoU. This suggests that the model has the potential to achieve much better performance if provided with additional training data. However, the loss behavior is not ideal, as it increases during the final epochs, ultimately triggering the early stopping rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX2OmNDil4NI"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaCuCz4c2kSg"
      },
      "outputs": [],
      "source": [
        "#test function\n",
        "def test_model_attention_unet(model,loss_function, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    iou_scores_test = 0\n",
        "    with torch.no_grad():\n",
        "        for step, (images, masks) in enumerate(test_loader):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs_test = model(images)\n",
        "            loss=loss_function(outputs_test,masks)\n",
        "            test_loss += loss.item()\n",
        "            threshold = 0.55\n",
        "            prediction_test = (outputs_test > threshold)\n",
        "            iou_my=iou_score(prediction_test, masks)\n",
        "            iou_my=iou_my.cpu().numpy()\n",
        "            iou_scores_test+=iou_my\n",
        "    iou_scores_test_final=iou_scores_test/len(test_loader)\n",
        "    test_loss_final = test_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {test_loss_final:.4f}, Test IOU: {iou_scores_test_final}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEVvDQsHEWr8"
      },
      "outputs": [],
      "source": [
        "test_model_attention_unet(model,bce_dice_loss,testing_loader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSm0HMi9E4Dy"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for step, (images, masks) in enumerate(testing_loader):\n",
        "        if step == 1:\n",
        "            break\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs_test = model(images)\n",
        "        threshold = 0.55\n",
        "        prediction_test = (outputs_test > threshold)\n",
        "        np_images_original = images.cpu().detach().numpy()\n",
        "        np_images = np.squeeze(np_images_original, axis=1)\n",
        "        np_mask=np.squeeze(masks.cpu().detach().numpy(),axis=1)\n",
        "        np_prediction_test = np.squeeze(prediction_test.cpu().detach().numpy(), axis=1)\n",
        "        outputs_np = np.squeeze(outputs_test.cpu().detach().numpy(), axis=1)\n",
        "\n",
        "print(np_images.shape)\n",
        "print(np_mask.shape)\n",
        "print(np_prediction_test.shape)\n",
        "print(outputs_np.shape)\n",
        "\n",
        "#plotting the predicted mask , predicted binary mask and the mask on original images\n",
        "fig, axes = plt.subplots(4, 3, figsize=(15, 25))\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "for i in range(4):\n",
        "    # Original image with predicted mask\n",
        "    axes[i, 0].imshow(np_images[i])\n",
        "    axes[i, 0].imshow(outputs_np[i], alpha=0.5, cmap='copper')\n",
        "    axes[i, 0].set_title(\"Original Image with Predicted Mask\")\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Original image with original mask\n",
        "    axes[i, 1].imshow(np_images[i])\n",
        "    axes[i, 1].imshow(np_mask[i], alpha=0.5, cmap='copper')\n",
        "    axes[i, 1].set_title(\"Original Image with Original Mask\")\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "    # Original image with binary mask\n",
        "    axes[i, 2].imshow(np_images[i])\n",
        "    axes[i, 2].imshow(np_prediction_test[i], alpha=0.5, cmap='copper')\n",
        "    axes[i, 2].set_title(\"Original Image with Binary Mask\")\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4eQOTz3vbhd"
      },
      "source": [
        "The results on the validation data outperform those on the training data in terms of IoU. This suggests that the model has the potential to achieve much better performance if provided with additional training data. However, the loss behavior is not ideal, as it increases during the final epochs, ultimately triggering the early stopping rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFz2bnzawAd4"
      },
      "source": [
        "The results on the test data are reasonably good, as indicated by the IoU score. In the selected images, we observe some accurate \"hits\" in the predicted masks. While the predictions are not perfect, they are promising and demonstrate potential for further improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csaqJv-CZ5d2"
      },
      "source": [
        "# CGANs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EESH-zuwQIn"
      },
      "outputs": [],
      "source": [
        "device='cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y09pPtKmvOD2"
      },
      "outputs": [],
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv4nztPNs1bu"
      },
      "outputs": [],
      "source": [
        "class Dataset_gan(torch.utils.data.Dataset):\n",
        "    def __init__(self, images: list, labels: list, transform=None):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('L')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E3k3lM1_PKY"
      },
      "outputs": [],
      "source": [
        "class c_generator(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super().__init__()\n",
        "        self.label_conditioned_generator = nn.Sequential(nn.Embedding(n_classes, 100), nn.Linear(100, 16))#for the class input\n",
        "        self.latent = nn.Sequential(nn.Linear(100, 4*4*512),\n",
        "                                    nn.LeakyReLU(0.2, inplace=True))#for the noise input\n",
        "        self.upconv_block = nn.Sequential(nn.ConvTranspose2d(513, 64*16, 4, 2, 1, bias=False),\n",
        "                                          nn.BatchNorm2d(64*16),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.ConvTranspose2d(64*16, 64*8, 4, 2, 1, bias=False),\n",
        "                                          nn.BatchNorm2d(64*8),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.ConvTranspose2d(64*8, 64*4, 4, 2, 1, bias=False),\n",
        "                                          nn.BatchNorm2d(64*4),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.ConvTranspose2d(64*4, 64*2, 4, 2, 1, bias=False),\n",
        "                                          nn.BatchNorm2d(64*2),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.ConvTranspose2d(64*2, 64, 4, 2, 1, bias=False),\n",
        "                                          nn.BatchNorm2d(64),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False))\n",
        "        self.seg = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, noise, class_my):\n",
        "        label_output = self.label_conditioned_generator(class_my)\n",
        "        #print('lable shape ',label_output.shape)\n",
        "        noise_output = self.latent(noise)\n",
        "        #print('noise.shape',noise_output.shape)\n",
        "        label_output = label_output.view(-1, 1, 4, 4)\n",
        "        #print('lable out put shape after view',label_output.shape)\n",
        "        noise_output = noise_output.view(-1, 512, 4, 4)\n",
        "        concate = torch.cat((noise_output, label_output), dim=1)\n",
        "        #print('concate shape ',concate.shape)\n",
        "        output = self.upconv_block(concate)\n",
        "        output = self.seg(output)\n",
        "        return output\n",
        "\n",
        "#my_generator=c_generator(3)\n",
        "#n_classes = 3  # Assuming the range is 0 to 2\n",
        "#tensor = torch.randint(0, n_classes, (4,))\n",
        "#print(tensor)\n",
        "#generate the random  noise and the class tensor if batch size 4\n",
        "#noise=torch.randn(4,100)\n",
        "#print(noise.shape)\n",
        "#print(tensor.shape)\n",
        "#output=my_generator(noise,tensor)\n",
        "#print(output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAMOPSQYfqFt"
      },
      "outputs": [],
      "source": [
        "class c_discriminator(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super().__init__()\n",
        "        self.label_conditioned_discriminator=nn.Sequential(nn.Embedding(n_classes,100),nn.Linear(100,65536))\n",
        "        self.conv_block_1=nn.Sequential(nn.Conv2d(2,64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                                    nn.Conv2d(64,128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                    nn.BatchNorm2d(128,momentum=0.1,  eps=0.8),\n",
        "                                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                                    nn.Conv2d(128,256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                    nn.BatchNorm2d(256,momentum=0.1,  eps=0.8),\n",
        "                                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                                    nn.Conv2d(256,512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                    nn.BatchNorm2d(512,momentum=0.1,  eps=0.8),\n",
        "                                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                                    nn.Dropout(0.4),\n",
        "                                    nn.Flatten(),\n",
        "                                    nn.Linear(131072,1),\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "    def forward(self,image_tesnsor,class_my):\n",
        "      label_output=self.label_conditioned_discriminator(class_my)\n",
        "      #print('lable output shape',label_output.shape)\n",
        "      label_output=label_output.view(-1,1,256,256)\n",
        "      #print('lable output shape after view',label_output.shape)\n",
        "      concate=torch.cat((image_tesnsor,label_output),dim=1)#shape[batch_size,2,256,256]\n",
        "      #print('concate shape',concate.shape)\n",
        "      output=self.conv_block_1(concate)\n",
        "\n",
        "      return output\n",
        "\n",
        "#my_dis=c_discriminator(3)\n",
        "#n_classes = 3  # Assuming the range is 0 to 2\n",
        "#tensor = torch.randint(0, n_classes, (4,))\n",
        "#print(tensor)\n",
        "#generate the random  noise and the class tensor if batch size 4\n",
        "#noise=torch.randn(4,1,256,256)\n",
        "#print(noise.shape)\n",
        "#print(tensor.shape)\n",
        "#output=my_dis(noise,tensor)\n",
        "#print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz5zThON5S4y"
      },
      "outputs": [],
      "source": [
        "my_generator=c_generator(3)\n",
        "my_generator.to(device)\n",
        "my_discriminaor=c_discriminator(3)\n",
        "my_discriminaor.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpiWv6JEy2Ak"
      },
      "outputs": [],
      "source": [
        "bc_loss=nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "18N4RFDWy6h5"
      },
      "outputs": [],
      "source": [
        "G_optimizer=torch.optim.Adam(params=my_generator.parameters(), lr=0.0001,weight_decay=1e-3 )\n",
        "D_optimizer=torch.optim.Adam(params=my_discriminaor.parameters(), lr=0.0001,weight_decay=1e-3 )\n",
        "batch_size=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ8_EVVE7ag8"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(label, output):\n",
        "    disc_loss = bc_loss(output,label )\n",
        "    #print(total_loss)\n",
        "    return disc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuUUdu4e82Zt"
      },
      "outputs": [],
      "source": [
        "def generator_loss(label, fake_output):\n",
        "    gen_loss = bc_loss(label, fake_output)\n",
        "    return gen_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COsvP3u05_Nr"
      },
      "outputs": [],
      "source": [
        "transform_gan = T.Compose([\n",
        "    T.Resize([256,256]),\n",
        "    T.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xjrO7H4EprIR"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)  # 80% train, 20% tem\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
        "train_df_2=Dataset_gan(images=train_df['file_path'].values,labels=train_df['label'].values,transform=transform_gan)\n",
        "test_df_2=Dataset_gan(images=test_df['file_path'].values,labels=test_df['label'].values,transform=transform_gan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8agckWnA5uHI"
      },
      "outputs": [],
      "source": [
        "#check the image shape\n",
        "rand_img=train_df_2[1][0].shape\n",
        "rand_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAntwlrzF-WK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsojOIEBpxCg"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_params = {'batch_size': 4,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "                'pin_memory':True\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 4,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0,\n",
        "               'pin_memory':True\n",
        "                }\n",
        "training_loader = DataLoader(train_df_2, **train_params)\n",
        "testing_loader = DataLoader(test_df_2, **test_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vIRmOr0TsIa"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCXdgxbQw-9D"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "def training_gan(generator,discriminator,batch_size,optimizer_g,optimizer_d,loss_function_G,loss_function_D,noise_dim,train_loader,epochs,device):\n",
        "  G_loses=[]\n",
        "  D_loses=[]\n",
        "  #best_val_loss = float('inf')\n",
        "  #no_improvement_count = 0\n",
        "  for epoch in range(epochs):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    train_loss_g = 0\n",
        "    train_loss_d = 0\n",
        "    step = 0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for step, (images,labeles ) in progress_bar:\n",
        "      images, labeles = images.to(device), labeles.to(device)\n",
        "      print(\"the shape of the images\",images.shape)\n",
        "      #making sure that labels are tensors of shape [batch size,labels]\n",
        "      labeles=labeles.type(torch.LongTensor)\n",
        "      labeles=labeles.to(device)\n",
        "      print(\"the sahpe of the clases\",labeles.shape)\n",
        "      print('the lables',labeles)\n",
        "      optimizer_g.zero_grad()\n",
        "      optimizer_d.zero_grad()\n",
        "      #create a random  noise tensor of shapr[batch_size,100]\n",
        "      my_noise=torch.randn(batch_size, noise_dim)\n",
        "      my_noise=my_noise.to(device)\n",
        "      #creating the tensor of 1 for the \"original\" images\n",
        "      real_labels=torch.ones(batch_size,1)\n",
        "      real_labels=real_labels.to(device)\n",
        "      #creating the tensor of 0 for the \"fake images\"\n",
        "      fake_labels=torch.zeros(batch_size,1)\n",
        "      fake_labels=fake_labels.to(device)\n",
        "      #the discrimiator loss for real images\n",
        "      real_outputs=discriminator(images,labeles)#probability\n",
        "      print('prob of dic real',real_outputs)\n",
        "      real_loss=loss_function_D(real_labels,real_outputs)\n",
        "      #Creating fake images from  the noise\n",
        "      fake_g_images=generator(my_noise,labeles)\n",
        "      #the discriminator loss from the fake images\n",
        "      fake_outputs=discriminator(fake_g_images.detach(),labeles)#probability\n",
        "      fake_loss=loss_function_D(fake_labels,fake_outputs)\n",
        "      total_loss_d=(fake_loss+real_loss)/2\n",
        "      train_loss_d+=total_loss_d.item()\n",
        "      total_loss_d.backward()\n",
        "      optimizer_d.step()\n",
        "      #the generator loss from the outputs of the discriminator on the fake images\n",
        "      g_loss=loss_function_G(real_labels,discriminator(fake_g_images,labeles))\n",
        "      train_loss_g+=g_loss.item()\n",
        "      g_loss.backward()\n",
        "      optimizer_g.step()\n",
        "      step += 1\n",
        "      if step % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Step {step}, Loss G: {train_loss_g/step:.4f}, Loss D: {train_loss_d/step:.4f}\")\n",
        "    train_loss_g_epoch = train_loss_g/step\n",
        "    train_loss_d_epoch = train_loss_d/step\n",
        "    G_loses.append(train_loss_g_epoch)\n",
        "    D_loses.append(train_loss_d_epoch)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss G: {train_loss_g_epoch:.4f}, Training Loss D: {train_loss_d_epoch}\")\n",
        "  #plotting\n",
        "  plt.figure(figsize=(20,8))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.title(\"Model Loss\")\n",
        "  plt.plot(G_loses, label=\"Generator\")\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.title(\"Model Loss\")\n",
        "  plt.plot(D_loses, label=\"Discriminator\")\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXAg2s1erODn"
      },
      "outputs": [],
      "source": [
        "#lets train\n",
        "training_gan(my_generator,my_discriminaor,4,G_optimizer,D_optimizer,generator_loss,discriminator_loss,100,training_loader,50,device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOjSOhG1WGmrDLieDfu1fqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}